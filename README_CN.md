# 为复杂问题解决而设计的智能体系统

这也是我参加阿里巴巴数学竞赛AI挑战赛的解决方案。让我惊讶的是，相比其他使用复杂多智能体系统，它如此简单却取得了如此好的结果。

我先分享一下大致的思路：
我是让多种LLMs进行多轮寻求最优解的“自我拷问”，借鉴辩论的思想，逐步迭代当前解题过程，最终选出最优解，给出结论。

这个系统包含两套模型：主模型/学生模型和评估模型/教师模型。整个流程大致如下:
![image](https://github.com/richards199999/Agent-System-for-Complex-Promblem-Solving/assets/142148415/6df5733c-f447-42f6-b98a-3fe6aa0f6363)
1. 主模型（如GPT-4 Turbo或Claude-3 Opus）根据详细的指令，分步解答数学问题：
   - 列出相关知识点
   - 写下初步思考过程
   - 展开具体计算过程（无跳步）
   - 给出最终结果
2. 主模型的答案提交给评估模型审阅。评估模型也分步进行：
   - 总览检查是否有明显漏洞，给出初步反馈
   - 仔细检查计算过程，找出可能的错误
   - 评判推理逻辑是否严谨
   - 总结所有反馈意见
3. 评估模型的反馈意见返回给主模型，由主模型参考修正答案。
4. 反复迭代上述过程数次（如5次），使答案不断完善。
5. 分别用两套模型各自迭代，最后得到两份修正后的答案。
6. 用GPT-4 Turbo对两份答案从计算、逻辑和清晰度等方面打分比较，给出最佳答案。

当然这个方案也有很大的改善空间，我也和特工宇宙团队成员和SuperCarryMan团队成员交流了一下，做了一点小小的总结和思考：
1. 无论是哪种形式的Agent方案，都应该保证Sub-agent之间的信息的传达与推理不能出错，要保证这一点，我认为需要有一个全局的“监管者”，能够实时地看到Agent流程，遇到问题能及时干预，以免由于某个节点出错而导致错误放大；
2. 如果系统在概念梳理阶段，将关联概念的知识点用RAG从预设好的知识库中提取出来；且在计算时，使用外部工具进行计算，那结果必定会比仅LLM的方案要好；
3. 在复杂任务中，Agent或类Agent架构显得更加重要且有效；
4. 目前在Math和Reasoning上具有突出能力的开源、闭源模型不多；且模型在这两方面的能力仍需加强。
